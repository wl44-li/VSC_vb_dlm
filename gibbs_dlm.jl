include("kl_optim.jl")

begin
	using Distributions, Random
	using LinearAlgebra
	using StatsFuns
	using MCMCChains
	using Statistics
	using DataFrames
	using StatsPlots
	using SpecialFunctions
	using PDMats
	using StatsBase
	using Dates
	using DataFrames
end

function forward_filter(Ys, A, C, R, Q, m_0, C_0)
	_, T = size(Ys)
	d, _ = size(A)
	
	# Initialize, using "DLM with R" notation
	ms = zeros(d, T+1)
	Cs = zeros(d, d, T+1)

	ms[:, 1] = m_0
	Cs[:, :, 1] = C_0
	
	# one-step ahead latent distribution, used in backward sampling
	as = zeros(d, T)
	Rs = zeros(d, d, T)

	for t in 1:T
		# Prediction
		as[:, t] = a_t = A * ms[:, t]
		Rs[:, :, t] = R_t = A * Cs[:, :, t] * A' + Q #W
		
		# Update
		f_t = C * a_t
		S_t = C * R_t * C' + R #V

		# filter 
		ms[:, t+1] = a_t + R_t * C' * inv(S_t) * (Ys[:, t] - f_t)
		Cs[:, :, t+1]= R_t - R_t * C' * inv(S_t) * C * R_t
	end
	return ms, Cs, as, Rs
end


function ffbs_x(Ys, A, C, R, Q, m_0, P_0)
	d, _ = size(A)
	_, T = size(Ys)

	ms, Cs, as, Rs = forward_filter(Ys, A, C, R, Q, m_0, P_0)
	X = zeros(d, T+1)

	# DEBUG FFBS
	try
		X[:, end] = rand(MvNormal(ms[:, end], Symmetric(Cs[:, :, end])))
	catch PosDefException
		println("Pathology case encountered: ")
		println("C_end: ")
		println(Cs[:, :, end])
		println("Y_end:")
		println(Ys[:, end])
	end

	# backward sampling
	for t in T:-1:1
		h_t = ms[:, t] + Cs[:, :, t] * A' * inv(Rs[:, :, t])*(X[:, t+1] - as[:, t])
		H_t = Cs[:, :, t] - Cs[:, :, t] * A' * inv(Rs[:, :, t]) * A * Cs[:, :, t]
		X[:, t] = rand(MvNormal(h_t, Symmetric(H_t)))
	end
	return X
end


# Multi-variate DLM with full unknown $R, Q$
function sample_R_(y, x, C, v_0, S_0)
    T = size(y, 2)
	
    residuals = [y[:, t] - C * x[:, t] for t in 1:T]
	SS_y = sum([residuals[t] * residuals[t]' for t in 1:T])
	
    scale_posterior = S_0 + SS_y .* 0.5
    v_p = v_0 + 0.5 * T

	S_p = PDMat(Symmetric(inv(scale_posterior)))
	Râ»Â¹ = rand(Wishart(v_p, S_p))
    return inv(Râ»Â¹)
end

# not-yet used 
# function sample_Q_(x, A, v_1, S_1, x_0)
#     T = size(x, 2)
	
#     residuals = [x[:, t] - A * x[:, t-1] for t in 2:T]
# 	SS_1 = sum([residuals[t] * residuals[t]' for t in 1:T-1])
#     scale_posterior = S_1 + SS_1 .* 0.5

# 	scale_posterior += (x[:, 1] - A * x_0) * (x[:, 1] - A * x_0)' .* 0.5
#     v_p = v_1 + 0.5 * T
# 	S_p = PDMat(Symmetric(inv(scale_posterior)))

# 	Qâ»Â¹ = rand(Wishart(v_p, S_p))
#     return inv(Qâ»Â¹)
# end

function gibbs_dlm_cov(y, A, C, mcmc=3000, burn_in=1500, thinning=1, debug=false)
	P, T = size(y)
	K = size(A, 2)
	
	Î¼_0 = vec(mean(y, dims=2)) 
	Î»_0 = Matrix{Float64}(I, K, K)
	
	v_0 = P + 1.0 
	S_0 = Matrix{Float64}(0.01 * I, P, P)

	# Initial values for the parameters
	Râ»Â¹ = rand(Wishart(v_0, inv(S_0)))
	R = inv(Râ»Â¹)

	# DEBUG: hyper-prior [Pathological Gamma(0.01, 0.01)], (0.1, 0.1)?
	Î±, Î² = 100, 100
	Ï_q = rand(Gamma(Î±, Î²), K)
    Q = Diagonal(1 ./ Ï_q)
	
	n_samples = Int.(mcmc/thinning)
	# Store the samples
	samples_X = zeros(n_samples, K, T)
	samples_Q = zeros(n_samples, K, K)
	samples_R = zeros(n_samples, P, P)
	
	# Gibbs sampler
	for i in 1:mcmc+burn_in
	    # Update the states

		if debug
			println("MCMC full R debug, iteration: $i")
		end
		x = ffbs_x(y, A, C, R, Q, Î¼_0, Î»_0)
		x = x[:, 2:end]

		# Update the system noise
		Q = sample_Q(x, A, Î±, Î²)

	    # Update the observation noise
		R = sample_R_(y, x, C, v_0, S_0)
	
	    # Store the samples
		if i > burn_in && mod(i - burn_in, thinning) == 0
			index = div(i - burn_in, thinning)
		    samples_X[index, :, :] = x
			samples_Q[index, :, :] = Q
		    samples_R[index, :, :] = R
		end
	end

	return samples_X, samples_Q, samples_R
end

# VBEM for general Q, R
begin
	struct Prior
	    Î½_R::Float64
	    W_R::Array{Float64, 2}
	    Î½_Q::Float64
	    W_Q::Array{Float64, 2}
	    Î¼_0::Array{Float64, 1}
	    Î›_0::Array{Float64, 2}
	end

	struct Q_Wishart
		Î½_R_q
		W_R_q
		Î½_Q_q
		W_Q_q
	end
end

function vb_m_step(y::Array{Float64, 2}, hss::HSS, prior::Prior, A::Array{Float64, 2}, C::Array{Float64, 2})
    _, T = size(y)
    
    # Compute the new parameters for the variational posterior of Î›_R
    Î½_Rn = prior.Î½_R + T
	W_Rn_inv = inv(prior.W_R) + y*y' - hss.S_C * C' - C * hss.S_C' + C * hss.W_C * C'
    
    # Compute the new parameters for the variational posterior of Î›_Q
    Î½_Qn = prior.Î½_Q + T
	W_Qn_inv = inv(prior.W_Q) + hss.W_C - hss.S_A * A' - A * hss.S_A' + A * hss.W_A * A'

	# Return expectations for E-step, Eq[R], E_q[Q], co-variance matrices
	return W_Rn_inv ./ Î½_Rn, W_Qn_inv ./ Î½_Qn, Q_Wishart(Î½_Rn, inv(W_Rn_inv), Î½_Qn, inv(W_Qn_inv))
	#return Î½_Rn .* inv(W_Rn_inv), Î½_Qn .* inv(W_Qn_inv) # precision matrices
end

function forward_(y::Array{Float64, 2}, A::Array{Float64, 2}, C::Array{Float64, 2}, E_R::Array{Float64, 2}, E_Q::Array{Float64, 2}, prior::Prior)
    _, T = size(y)
    K = size(A, 1)
    
    # Unpack the prior parameters
    Î¼_0, Î›_0 = prior.Î¼_0, prior.Î›_0
    
    # Initialize the filtered means and covariances
    Î¼_f = zeros(K, T)
    Î£_f = zeros(K, K, T)
    f_s = zeros(K, T)
	S_s = zeros(K, K, T)
    # Set the initial filtered mean and covariance to their prior values
	A_1 = A * Î¼_0
	R_1 = A * inv(Î›_0) * A' + E_Q
	
	f_1 = C * A_1
	S_1 = C * R_1 * C' + E_R
	f_s[:, 1] = f_1
	S_s[:, :, 1] = S_1
	
    Î¼_f[:, 1] = A_1 + R_1 * C'* inv(S_1) * (y[:, 1] - f_1)
    Î£_f[:, :, 1] = R_1 - R_1*C'*inv(S_1)*C*R_1 
    
    # Forward pass (kalman filter)
    for t = 2:T
        # Predicted state mean and covariance
        Î¼_p = A * Î¼_f[:, t-1]
        Î£_p = A * Î£_f[:, :, t-1] * A' + E_Q

		# marginal y - normalization
		f_t = C * Î¼_p
		S_t = C * Î£_p * C' + E_R
		f_s[:, t] = f_t
		S_s[:, :, t] = S_t
		
		# Filtered state mean and covariance (2.8a - 2.8c DLM with R)
		Î¼_f[:, t] = Î¼_p + Î£_p * C' * inv(S_t) * (y[:, t] - f_t)
		Î£_f[:, :, t] = Î£_p - Î£_p * C' * inv(S_t) * C * Î£_p
			
		# Kalman gain
        #K_t = Î£_p * C' / (C * Î£_p * C' + E_R)
        #Î¼_f[:, t] = Î¼_p + K_t * (y[:, t] - C * Î¼_p)
        #Î£_f[:, :, t] = (I - K_t * C) * Î£_p
    end
	
    log_z = sum(logpdf(MvNormal(f_s[:, i], Symmetric(S_s[:, :, i])), y[:, i]) for i in 1:T)
    return Î¼_f, Î£_f, log_z
end

function backward_(Î¼_f::Array{Float64, 2}, Î£_f::Array{Float64, 3}, A::Array{Float64, 2}, E_Q::Array{Float64, 2})
    K, T = size(Î¼_f)
    
    # Initialize the smoothed means, covariances, and cross-covariances
    Î¼_s = zeros(K, T)
    Î£_s = zeros(K, K, T)
    Î£_s_cross = zeros(K, K, T)
    
    # Set the final smoothed mean and covariance to their filtered values
    Î¼_s[:, T] = Î¼_f[:, T]
    Î£_s[:, :, T] = Î£_f[:, :, T]
    
    # Backward pass
    for t = T-1:-1:1
        # Compute the gain J_t
        J_t = Î£_f[:, :, t] * A' / (A * Î£_f[:, :, t] * A' + E_Q)

        # Update the smoothed mean Î¼_s and covariance Î£_s
        Î¼_s[:, t] = Î¼_f[:, t] + J_t * (Î¼_s[:, t+1] - A * Î¼_f[:, t])
        Î£_s[:, :, t] = Î£_f[:, :, t] + J_t * (Î£_s[:, :, t+1] - A * Î£_f[:, :, t] * A' - E_Q) * J_t'

        # Compute the cross covariance Î£_s_cross
        #Î£_s_cross[:, :, t+1] = inv(inv(Î£_f[:, :, t]) + A'*A) * A' * Î£_s[:, :, t+1]
		Î£_s_cross[:, :, t+1] = J_t * Î£_s[:, :, t+1]
    end
	
	Î£_s_cross[:, :, 1] = inv(I + A'*A) * A' * Î£_s[:, :, 1]
	#J_1 = I * A' / (A * I * A' + E_Q)
	#Î£_s_cross[:, :, 1] = J_1 * Î£_s[:, :, 1]
    return Î¼_s, Î£_s, Î£_s_cross
end

function vb_e_step(y::Array{Float64, 2}, A::Array{Float64, 2}, C::Array{Float64, 2}, E_R::Array{Float64, 2}, E_Q::Array{Float64, 2}, prior::Prior)
    # Run the forward pass
    Î¼_f, Î£_f, log_Z = forward_(y, A, C, E_R, E_Q, prior)

    # Run the backward pass
    Î¼_s, Î£_s, Î£_s_cross = backward_(Î¼_f, Î£_f, A, E_Q)

    # Compute the hidden state sufficient statistics
    W_C = sum(Î£_s, dims=3)[:, :, 1] + Î¼_s * Î¼_s'
    W_A = sum(Î£_s[:, :, 1:end-1], dims=3)[:, :, 1] + Î¼_s[:, 1:end-1] * Î¼_s[:, 1:end-1]'
    #S_C = y * Î¼_s'
	S_C = Î¼_s * y'
    S_A = sum(Î£_s_cross, dims=3)[:, :, 1] + Î¼_s[:, 1:end-1] * Î¼_s[:, 2:end]'

	# Return the hidden state sufficient statistics
    return HSS(W_C, W_A, S_C, S_A), log_Z
end

function vbem_(y::Array{Float64, 2}, A::Array{Float64, 2}, C::Array{Float64, 2}, prior::Prior, max_iter=300)
	
	hss = HSS(ones(size(A)), ones(size(A)), ones(size(C)), ones(size(A)))
	E_R, E_Q  = missing, missing
	
	for i in 1:max_iter
		E_R, E_Q, _ = vb_m_step(y, hss, prior, A, C)
				
		hss, _ = vb_e_step(y, A, C, E_R, E_Q, prior)
	end

	return E_R, E_Q
end

function vbem_his_plot(y::Array{Float64, 2}, A::Array{Float64, 2}, C::Array{Float64, 2}, prior::Prior, max_iter=100)
    P, T = size(y)
    K, _ = size(A)

    W_C = zeros(K, K)
    W_A = zeros(K, K)
    S_C = zeros(P, K)
    S_A = zeros(K, K)
    hss = HSS(W_C, W_A, S_C, S_A)
	E_R, E_Q  = missing, missing
	
    # Initialize the history of E_R and E_Q
    E_R_history = zeros(P, P, max_iter)
    E_Q_history = zeros(K, K, max_iter)

    # Repeat until convergence
    for iter in 1:max_iter
		E_R, E_Q, _ = vb_m_step(y, hss, prior, A, C)
				
		hss, _ = vb_e_step(y, A, C, E_R, E_Q, prior)

        # Store the history of E_R and E_Q
        E_R_history[:, :, iter] = E_R
        E_Q_history[:, :, iter] = E_Q
    end

	p1 = plot(title = "Learning of R")

	# Show progress of diagonal entries
    for i in 1:P
        plot!(10:max_iter, [E_R_history[i, i, t] for t in 10:max_iter], label = "R[$i, $i]")
    end

    p2 = plot(title = "Learning of Q")
    for i in 1:K
        plot!(10:max_iter, [E_Q_history[i, i, t] for t in 10:max_iter], label = "Q[$i, $i]")
    end
	
	function_name = "R_Q_Progress"
	p = plot(p1, p2, layout = (1, 2))
	plot_file_name = "$(splitext(basename(@__FILE__))[1])_$(function_name)_$(Dates.format(now(), "yyyymmdd_HHMMSS")).svg"
    savefig(p, joinpath(expanduser("~/Downloads/_graphs"), plot_file_name))
end

# With Convergence Check
function vbem_c(y::Array{Float64, 2}, A::Array{Float64, 2}, C::Array{Float64, 2}, prior::Prior, max_iter=1000, tol=5e-3)
	hss = HSS(ones(size(A)), ones(size(A)), ones(size(C)), ones(size(A)))
	E_R, E_Q  = missing, missing
	elbo_prev = -Inf
	el_s = zeros(max_iter)
	for i in 1:max_iter
		E_R, E_Q, Q_Wi = vb_m_step(y, hss, prior, A, C)
				
		hss, log_Z = vb_e_step(y, A, C, E_R, E_Q, prior)

		kl_Wi = kl_Wishart(Q_Wi.Î½_R_q, Q_Wi.W_R_q, prior.Î½_R, prior.W_R) + kl_Wishart(Q_Wi.Î½_Q_q, Q_Wi.W_Q_q, prior.Î½_Q, prior.W_Q)
		elbo = log_Z - kl_Wi
		el_s[i] = elbo
		
		if abs(elbo - elbo_prev) < tol
			println("Stopped at iteration: $i")
			el_s = el_s[1:i]
            break
		end
		
        elbo_prev = elbo

		if (i == max_iter)
			println("Warning: VB have not necessarily converged at $max_iter iterations with tolerance $tol")
		end
	end

	return E_R, E_Q, el_s
end

# Restrict R, Q as diagonal matrices
function vb_m_diag(y, hss::HSS, hpp::HPP_D, A::Array{Float64, 2}, C::Array{Float64, 2})
    D, T = size(y)
    K = size(A, 1)
    # Compute the new parameters for the variational posterior of Î›_R
	G = y*y' - hss.S_C * C' - C * hss.S_C' + C * hss.W_C * C'
    a_ = hpp.a + 0.5 * T
	a_s = a_ * ones(D)
    b_s = [hpp.b + 0.5 * G[i, i] for i in 1:D]
	q_Ï = Gamma.(a_s, 1 ./ b_s)
	Exp_Râ»Â¹ = diagm(mean.(q_Ï))
	
    # Compute the new parameters for the variational posterior of Î›_Q
    H = hss.W_C - hss.S_A * A' - A * hss.S_A' + A * hss.W_A * A'
	Î±_ = hpp.Î± + 0.5 * T
	Î±_s = Î±_ * ones(K)
    Î²_s = [hpp.Î² + 0.5 * H[i, i] for i in 1:K]
	q_ð› = Gamma.(Î±_s, 1 ./ Î²_s)	
	Exp_Qâ»Â¹= diagm(mean.(q_ð›))
	
	return Exp_Râ»Â¹, Exp_Qâ»Â¹, Q_Gamma(a_, b_s, Î±_, Î²_s)
end

function forward_v(y, A::Array{Float64, 2}, C::Array{Float64, 2}, E_R, E_Q, prior::HPP_D)
    P, T = size(y)
    K = size(A, 1)
    
    # Unpack the prior parameters
    Î¼_0, Î›_0 = prior.Î¼_0, prior.Î£_0
    
    # Initialize the filtered means and covariances
    Î¼_f = zeros(K, T)
    Î£_f = zeros(K, K, T)

    f_s = zeros(P, T)
	S_s = zeros(P, P, T)
	
    # Set the initial filtered mean and covariance to their prior values
	A_1 = A * Î¼_0
	R_1 = A * inv(Î›_0) * A' + E_Q
	
	f_1 = C * A_1
	S_1 = C * R_1 * C' + E_R
	f_s[:, 1] = f_1
	S_s[:, :, 1] = S_1
	
    Î¼_f[:, 1] = A_1 + R_1 * C'* inv(S_1) * (y[:, 1] - f_1)
    Î£_f[:, :, 1] = R_1 - R_1*C'*inv(S_1)*C*R_1 
    
    # Forward pass (kalman filter)
    for t = 2:T
        # Predicted state mean and covariance
        Î¼_p = A * Î¼_f[:, t-1]
        Î£_p = A * Î£_f[:, :, t-1] * A' + E_Q

		# marginal y - normalization
		f_t = C * Î¼_p
		S_t = C * Î£_p * C' + E_R
		f_s[:, t] = f_t
		S_s[:, :, t] = S_t
		
		# Filtered state mean and covariance (2.8a - 2.8c DLM with R)
		Î¼_f[:, t] = Î¼_p + Î£_p * C' * inv(S_t) * (y[:, t] - f_t)
		Î£_f[:, :, t] = Î£_p - Î£_p * C' * inv(S_t) * C * Î£_p
			
		# Kalman gain
        #K_t = Î£_p * C' / (C * Î£_p * C' + E_R)
        #Î¼_f[:, t] = Î¼_p + K_t * (y[:, t] - C * Î¼_p)
        #Î£_f[:, :, t] = (I - K_t * C) * Î£_p
    end
	
    log_z = sum(logpdf(MvNormal(f_s[:, i], Symmetric(S_s[:, :, i])), y[:, i]) for i in 1:T)
    return Î¼_f, Î£_f, log_z
end

function backward_v(Î¼_f::Array{Float64, 2}, Î£_f::Array{Float64, 3}, A::Array{Float64, 2}, E_Q::Array{Float64, 2}, prior)
    K, T = size(Î¼_f)
    
    # Initialize the smoothed means, covariances, and cross-covariances
    Î¼_s = zeros(K, T)
    Î£_s = zeros(K, K, T)
    Î£_s_cross = zeros(K, K, T)
    
    # Set the final smoothed mean and covariance to their filtered values
    Î¼_s[:, T] = Î¼_f[:, T]
    Î£_s[:, :, T] = Î£_f[:, :, T]
    
    # Backward pass
    for t = T-1:-1:1
        # Compute the gain J_t
        J_t = Î£_f[:, :, t] * A' / (A * Î£_f[:, :, t] * A' + E_Q)

        # Update the smoothed mean Î¼_s and covariance Î£_s
        Î¼_s[:, t] = Î¼_f[:, t] + J_t * (Î¼_s[:, t+1] - A * Î¼_f[:, t])
        Î£_s[:, :, t] = Î£_f[:, :, t] + J_t * (Î£_s[:, :, t+1] - A * Î£_f[:, :, t] * A' - E_Q) * J_t'

        # Compute the cross covariance Î£_s_cross
        #Î£_s_cross[:, :, t+1] = inv(inv(Î£_f[:, :, t]) + A'*A) * A' * Î£_s[:, :, t+1]
		Î£_s_cross[:, :, t+1] = J_t * Î£_s[:, :, t+1]
    end
	
	Î£_s_cross[:, :, 1] = inv(I + A'*A) * A' * Î£_s[:, :, 1]
	
	J_0 = prior.Î£_0 * A' / (A * prior.Î£_0 * A' + E_Q)
	Î¼_s0 = prior.Î¼_0 + J_0 * (Î¼_s[:, 1] -  A * prior.Î¼_0)
	Î£_s0 = prior.Î£_0 + J_0 * (Î£_s[:, :, 1] - A * prior.Î£_0 * A' - E_Q) * J_0'
    return Î¼_s, Î£_s, Î¼_s0, Î£_s0, Î£_s_cross
end

function vb_e_diag(y, A::Array{Float64, 2}, C::Array{Float64, 2}, E_R, E_Q, prior)
    # Run the forward pass
    Î¼_f, Î£_f, log_Z = forward_v(y, A, C, E_R, E_Q, prior)

    # Run the backward pass
    Î¼_s, Î£_s, Î¼_s0, Î£_s0, Î£_s_cross = backward_v(Î¼_f, Î£_f, A, E_Q, prior)

    # Compute the hidden state sufficient statistics
    W_C = sum(Î£_s, dims=3)[:, :, 1] + Î¼_s * Î¼_s'
    W_A = sum(Î£_s[:, :, 1:end-1], dims=3)[:, :, 1] + Î¼_s[:, 1:end-1] * Î¼_s[:, 1:end-1]'
	W_A += Î£_s0 + Î¼_s0*Î¼_s0'
	
    S_C = Î¼_s * y'
    S_A = sum(Î£_s_cross, dims=3)[:, :, 1] + Î¼_s[:, 1:end-1] * Î¼_s[:, 2:end]'
	S_A += Î¼_s0*Î¼_s[:, 1]'

	# Return the hidden state sufficient statistics
    return HSS(W_C, W_A, S_C, S_A), Î¼_s0, Î£_s0, log_Z
end

# VBEM with Convergence
function vbem_c_diag(y, A::Array{Float64, 2}, C::Array{Float64, 2}, prior, hp_learn=false, max_iter=500, tol=1e-3)

	D, _ = size(y)
	K = size(A, 1)
	hss = HSS(ones(size(A)), ones(size(A)), ones(size(C)), ones(size(A)))
	E_R_inv, E_Q_inv = missing, missing
	elbo_prev = -Inf
	el_s = zeros(max_iter)
	
	
	for i in 1:max_iter
		E_R_inv, E_Q_inv, Q_gam = vb_m_diag(y, hss, prior, A, C)
		hss, Î¼_s0, Î£_s0, log_Z = vb_e_diag(y, A, C, inv(E_R_inv), inv(E_Q_inv), prior)
		
		kl_Ï = sum([kl_gamma(prior.a, prior.b, Q_gam.a, (Q_gam.b)[s]) for s in 1:D])
		kl_ð› = sum([kl_gamma(prior.Î±, prior.Î², Q_gam.Î±, (Q_gam.Î²)[s]) for s in 1:K])
		
		elbo = log_Z - kl_Ï - kl_ð›
		el_s[i] = elbo

		if abs(elbo - elbo_prev) < tol
			println("Stopped at iteration: $i")
			el_s = el_s[1:i]
            break
		end
		
		if (hp_learn)
			if (i%5 == 0) 
				a_, b_, Î±_, Î²_ = update_hyp_D(prior, Q_gam)
				prior = HPP_D(Î±_, Î²_, a_, b_, Î¼_s0, Î£_s0)
			end
		end

        elbo_prev = elbo

		if (i == max_iter)
			println("Warning: VB have not necessarily converged at $max_iter iterations with tolerance $tol")
		end
	end
	
	return inv(E_R_inv), inv(E_Q_inv), el_s
end

# Gibbs sampling of diagonal co-variances
function sample_R(Xs, Ys, C, a_Ï, b_Ï)
    P, T = size(Ys)
    Ï_sampled = zeros(P)
    for i in 1:P
        Y = Ys[i, :]
        a_post = a_Ï + T / 2
        b_post = b_Ï + 0.5 * sum((Y' - C[i, :]' * Xs).^2)
		
        Ï_sampled[i] = rand(Gamma(a_post, 1 / b_post))
    end
    return diagm(1 ./ Ï_sampled)
end

function sample_Q(Xs, A, Î±_q, Î²_q)
    K, T = size(Xs)
    q_sampled = zeros(K)
    for i in 1:K
        X_diff = Xs[i, 2:end] - (A * Xs[:, 1:end-1])[i, :]
        Î±_post = Î±_q + T / 2 - 1  # Subtracting 1 as the first state doesn't have a predecessor
        Î²_post = Î²_q + 0.5 * sum(X_diff.^2)
        
        q_sampled[i] = rand(Gamma(Î±_post, 1 / Î²_post))
    end
    return diagm(1 ./ q_sampled)
end

function test_Gibbs_RQ()
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	Q = Diagonal([1.0, 1.0])
	R = Diagonal([0.1, 0.1])
	Î¼_0 = [0.0, 0.0]
	Î£_0 = Diagonal([1.0, 1.0])
	Random.seed!(123)
	T = 1000
	y, x_true = gen_data(A, C, Q, R, Î¼_0, Î£_0, T)
	prior = Q_Gamma(100, 100, 100, 100)

	R = sample_R(x_true, y, C, prior.a, prior.b)
	Q = sample_Q(x_true, A, prior.Î±, prior.Î²)

	println("R:")
	show(stdout, "text/plain", R)
	println()
	println("Q:")
	show(stdout, "text/plain", Q)
end

function gibbs_diag(y, A, C, prior::HPP_D, mcmc=3000, burn_in=1500, thinning=1, debug=false)
	P, T = size(y)
	K = size(A, 2)
	
	m_0 = prior.Î¼_0
	P_0 = prior.Î£_0
	
	a, b, Î±, Î² = prior.a, prior.b, prior.Î±, prior.Î²
	Ï_r = rand(Gamma(a, b), P)
    R = Diagonal(1 ./ Ï_r)
	Ï_q = rand(Gamma(Î±, Î²), K)
    Q = Diagonal(1 ./ Ï_q)

	n_samples = Int.(mcmc/thinning)
	# Store the samples
	samples_X = zeros(n_samples, K, T)
	samples_Q = zeros(n_samples, K, K)
	samples_R = zeros(n_samples, P, P)
	
	# Gibbs sampler
	for i in 1:mcmc+burn_in
		if debug
			println("MCMC diagonal R, Q debug, iteration : $i")
		end
	    # Update the states
		x = ffbs_x(y, A, C, R, Q, m_0, P_0)
		x = x[:, 2:end]

		# Update the system noise
		Q = sample_Q(x, A, Î±, Î²)
		
	    # Update the observation noise
		R = sample_R(x, y, C, a, b)
	
	    # Store the samples
		if i > burn_in && mod(i - burn_in, thinning) == 0
			index = div(i - burn_in, thinning)
		    samples_X[index, :, :] = x
			samples_Q[index, :, :] = Q
		    samples_R[index, :, :] = R
		end
	end

	return samples_X, samples_Q, samples_R
end

function test_gibbs_diag(y, x_true, mcmc=10000, burn_in=5000, thin=1)
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	K = size(A, 1)
	D, _ = size(y)
	prior = HPP_D(100, 100, 100, 100, zeros(K), Matrix{Float64}(I, K, K))
	n_samples = Int.(mcmc/thin)
	println("--- MCMC Diagonal Covariances ---")
	@time Xs_samples, Qs_samples, Rs_samples = gibbs_diag(y, A, C, prior, mcmc, burn_in, thin)
	println("\n--- n_samples: $n_samples, burn-in: $burn_in, thinning: $thin ---")
	Q_chain = Chains(reshape(Qs_samples, n_samples, K^2))
	R_chain = Chains(reshape(Rs_samples, n_samples, D^2))

	summary_stats_q = summarystats(Q_chain)
	summary_stats_r = summarystats(R_chain)
	summary_df_q = DataFrame(summary_stats_q)
	summary_df_r = DataFrame(summary_stats_r)

	summary_df_q = summary_df_q[[1, 4], :]
	summary_df_r = summary_df_r[[1, 4], :]
	println("Q summary stats: ", summary_df_q)
	println()
	println("R summary stats: ", summary_df_r)

	xs_m = mean(Xs_samples, dims=1)[1, :, :]
	println("\nMSE, MAD of MCMC X mean: ", error_metrics(x_true, xs_m))
end

function test_vb(y, x_true)
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	K = size(A, 1)
	prior = HPP_D(0.01, 0.01, 0.01, 0.01, zeros(K), Matrix{Float64}(I, K, K))
	println("\n--- VB with Diagonal Covariances ---")

	for t in [false, true]
		println("\nHyperparam optimisation: $t")
		@time R, Q, elbos = vbem_c_diag(y, A, C, prior, t)
		#p = plot(elbos, label = "elbo", title = "ElBO Progression, Hyperparam optim: $t")
		#display(p)
		#plot_file_name = "$(splitext(basename(@__FILE__))[1])_$(Dates.format(now(), "yyyymmdd_HHMMSS")).svg"
		#savefig(p, joinpath(expanduser("~/Downloads/_graphs"), plot_file_name))
		println("VB q(R): ")
		show(stdout, "text/plain", R)
		println("\n\n VB q(Q): ")
		show(stdout, "text/plain", Q)
		Î¼s_f, Ïƒs_f2 = forward_v(y, A, C, R, Q, prior)
		Î¼s_s, _, _ = backward_v(Î¼s_f, Ïƒs_f2, A, Q, prior)
		println("\n\nMSE, MAD of VB X: ", error_metrics(x_true, Î¼s_s))
		sleep(1)
	end

	D, _ = size(y)
	W_Q = Matrix{Float64}(I, K, K)
	W_R = Matrix{Float64}(I, D, D)
	prior = Prior(D + 1.0, W_R, K + 1.0, W_Q, zeros(K), Matrix{Float64}(I, K, K))
	println("\n--- VB with Full Co-variances ---")
	@time R, Q, elbos = vbem_c(y, A, C, prior)
	println("VB q(R): ")
	show(stdout, "text/plain", R)
	println("\n\nVB q(Q): ")
	show(stdout, "text/plain", Q)
	#p = plot(elbos, label = "elbo", title = "ElBO progression, seed = $rnd")
	#display(p)
	#plot_file_name = "$(splitext(basename(@__FILE__))[1])_$(Dates.format(now(), "yyyymmdd_HHMMSS")).svg"
	#savefig(p, joinpath(expanduser("~/Downloads/_graphs"), plot_file_name))
	Î¼s_f, Ïƒs_f2 = forward_(y, A, C, R, Q, prior)
    Î¼s_s, _, _ = backward_(Î¼s_f, Ïƒs_f2, A, Q)
	println("\n\nMSE, MAD of VB X: ", error_metrics(x_true, Î¼s_s))
end

function test_gibbs_cov(y, x_true, mcmc=10000, burn_in=5000, thin=1)
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	D, _ = size(y)
	K = size(A, 1)
	
	println("\n--- MCMC : R Full Covariances ---")
	n_samples = Int.(mcmc/thin)
	println("\n--- n_samples: $n_samples, burn-in: $burn_in, thinning: $thin ---")

	@time Xs_samples, Qs_samples, Rs_samples = gibbs_dlm_cov(y, A, C, mcmc, burn_in, thin)

	Q_chain = Chains(reshape(Qs_samples, n_samples, K^2))
	R_chain = Chains(reshape(Rs_samples, n_samples, D^2))

	summary_stats_q = summarystats(Q_chain)
	summary_stats_r = summarystats(R_chain)
	summary_df_q = DataFrame(summary_stats_q)
	summary_df_r = DataFrame(summary_stats_r)
	summary_df_q = summary_df_q[[1,4], :]
	println("Q summary stats: ", summary_df_q)
	println()
	println("R summary stats: ", summary_df_r)
	println()

	xs_m = mean(Xs_samples, dims=1)[1, :, :]
	println("MSE, MAD of MCMC X mean: ", error_metrics(x_true, xs_m))
	#println("MSE, MAD of MCMC X end: ", error_metrics(x_true, Xs_samples[end, :, :]))
end

function test_data(rnd, max_T = 500)
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	Q = Diagonal([1.0, 1.0]) # Diagonal Q
	R = [0.5 0.2; 0.2 0.5] # full-cov R
	Î¼_0 = [0.0, 0.0]
	Î£_0 = Diagonal([1.0, 1.0])
	Random.seed!(rnd)
	y, x_true = gen_data(A, C, Q, R, Î¼_0, Î£_0, max_T)
	return y, x_true
end

function test_ffbs()
	y, x_true = test_data(1)
	A = [1.0 0.0; 0.0 1.0]
	C = [1.0 0.0; 0.0 1.0]
	Q = Diagonal([1.0, 1.0])
	R = [0.5 0.2; 0.2 0.5]
	m_0 = [0.0, 0.0]
	P_0 = Diagonal([1.0, 1.0])
	_, Ps, _, _ = forward_filter(y, A, C, R, Q, m_0, P_0)
	println(Ps[:, :, end])
	X = ffbs_x(y, A, C, R, Q, m_0, P_0)

	X = X[:, 2:end]
	println("MSE, MAD of FFBS: ", error_metrics(x_true, X))
	plot_latent(x_true', X')
end

#test_ffbs()

function test_gibbs()
	seeds = [92, 134, 103, 133, 233]
	#108, 123, 
	for sd in seeds
		y, x_true = test_data(sd)
		println("--- Seed: $sd ---")
		test_gibbs_cov(y, x_true, 10000, 5000, 1)
		println()
		test_gibbs_diag(y, x_true, 10000, 5000, 1) # Debug: to investigate
	end
end

#test_gibbs()

function com_vb_gibbs()
	seeds = [108, 134, 123, 105, 233]
	#188, 199, 233, 234, 236
	for sd in seeds
		y, x_true = test_data(sd)
		println("--- Seed: $sd ---")
		test_gibbs_cov(y, x_true, 20000, 10000, 1)
		println()
		test_vb(y, x_true)
	end
end

function out_txt()
	file_name = "$(splitext(basename(@__FILE__))[1])_$(Dates.format(now(), "yyyymmdd_HHMMSS")).txt"

	open(file_name, "w") do f
		redirect_stdout(f) do
			redirect_stderr(f) do
				com_vb_gibbs()
			end	
		end
	end
end

out_txt()

# PLUTO_PROJECT_TOML_CONTENTS = """
# [deps]
# DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
# Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
# LinearAlgebra = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
# MCMCChains = "c7f686f2-ff18-58e9-bc7b-31028e88f75d"
# Optim = "429524aa-4258-5aef-a3af-852621145aeb"
# PDMats = "90014a1f-27ba-587c-ab20-58faa44d9150"
# Plots = "91a5bcdd-55d7-5caf-9e0b-520d859cae80"
# PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
# Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
# SpecialFunctions = "276daf66-3868-5448-9aa4-cd146d93841b"
# Statistics = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
# StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
# StatsFuns = "4c63d2b9-4356-54db-8cca-17b64c39e42c"
# StatsPlots = "f3b207a7-027a-5e70-b257-86293d7955fd"

# [compat]
# DataFrames = "~1.5.0"
# Distributions = "~0.25.96"
# MCMCChains = "~6.0.3"
# Optim = "~1.7.6"
# PDMats = "~0.11.17"
# Plots = "~1.38.16"
# PlutoUI = "~0.7.51"
# SpecialFunctions = "~2.2.0"
# StatsBase = "~0.34.0"
# StatsFuns = "~1.3.0"
# StatsPlots = "~0.15.5"
# """